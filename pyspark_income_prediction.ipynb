{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FeeKePxf2cvv",
        "outputId": "132374dd-0d50-481b-80e4-01dc2d6350ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "|  0|\n",
            "|  1|\n",
            "|  2|\n",
            "|  3|\n",
            "|  4|\n",
            "+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = (\n",
        "    SparkSession.builder\n",
        "    .appName(\"FinalProject_BigData\")\n",
        "    .master(\"local[*]\")\n",
        "    .getOrCreate()\n",
        ")\n",
        "spark.range(5).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1: Data Collection"
      ],
      "metadata": {
        "id": "vwRqjJJD5mAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
        "\n",
        "column_names = [\n",
        "    \"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\",\n",
        "    \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\",\n",
        "    \"capital_gain\", \"capital_loss\", \"hours_per_week\", \"native_country\", \"income\"\n",
        "]\n",
        "\n",
        "pandas_df = pd.read_csv(data_url, names=column_names, header=None, skipinitialspace=True)\n",
        "\n",
        "spark_df = spark.createDataFrame(pandas_df)\n",
        "\n",
        "display(spark_df)\n",
        "\n",
        "print(\"Initial preview of the dataset:\")\n",
        "spark_df.show(5)\n",
        "print(\"Total number of records:\", spark_df.count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "ZccGA--Y2jcG",
        "outputId": "de0429a6-2cfe-4af6-bc06-9eba1cc089de"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[age: bigint, workclass: string, fnlwgt: bigint, education: string, education_num: bigint, marital_status: string, occupation: string, relationship: string, race: string, sex: string, capital_gain: bigint, capital_loss: bigint, hours_per_week: bigint, native_country: string, income: string]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial preview of the dataset:\n",
            "+---+----------------+------+---------+-------------+------------------+-----------------+-------------+-----+------+------------+------------+--------------+--------------+------+\n",
            "|age|       workclass|fnlwgt|education|education_num|    marital_status|       occupation| relationship| race|   sex|capital_gain|capital_loss|hours_per_week|native_country|income|\n",
            "+---+----------------+------+---------+-------------+------------------+-----------------+-------------+-----+------+------------+------------+--------------+--------------+------+\n",
            "| 39|       State-gov| 77516|Bachelors|           13|     Never-married|     Adm-clerical|Not-in-family|White|  Male|        2174|           0|            40| United-States| <=50K|\n",
            "| 50|Self-emp-not-inc| 83311|Bachelors|           13|Married-civ-spouse|  Exec-managerial|      Husband|White|  Male|           0|           0|            13| United-States| <=50K|\n",
            "| 38|         Private|215646|  HS-grad|            9|          Divorced|Handlers-cleaners|Not-in-family|White|  Male|           0|           0|            40| United-States| <=50K|\n",
            "| 53|         Private|234721|     11th|            7|Married-civ-spouse|Handlers-cleaners|      Husband|Black|  Male|           0|           0|            40| United-States| <=50K|\n",
            "| 28|         Private|338409|Bachelors|           13|Married-civ-spouse|   Prof-specialty|         Wife|Black|Female|           0|           0|            40|          Cuba| <=50K|\n",
            "+---+----------------+------+---------+-------------+------------------+-----------------+-------------+-----+------+------------+------------+--------------+--------------+------+\n",
            "only showing top 5 rows\n",
            "Total number of records: 32561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2: Data Cleaning"
      ],
      "metadata": {
        "id": "wV6Ye9Iy5tju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, trim\n",
        "\n",
        "for col_name in spark_df.columns:\n",
        "    spark_df = spark_df.withColumn(col_name, trim(col(col_name)))\n",
        "\n",
        "spark_df = spark_df.replace(\"?\", None).dropna()\n",
        "\n",
        "numeric_fields = [\"age\", \"fnlwgt\", \"education_num\", \"capital_gain\", \"capital_loss\", \"hours_per_week\"]\n",
        "\n",
        "for field in numeric_fields:\n",
        "    spark_df = spark_df.withColumn(field, col(field).cast(\"int\"))\n",
        "\n",
        "display(spark_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "XreQuZhS2_XZ",
        "outputId": "a8a698d7-3a8b-443d-a225-ea66f47cbc5c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[age: int, workclass: string, fnlwgt: int, education: string, education_num: int, marital_status: string, occupation: string, relationship: string, race: string, sex: string, capital_gain: int, capital_loss: int, hours_per_week: int, native_country: string, income: string]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Here's what the cleaned data looks like:\")\n",
        "spark_df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvXYfAWd5pp7",
        "outputId": "fbc654af-1944-4cdc-a026-2e1cbc015d99"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's what the cleaned data looks like:\n",
            "+---+----------------+------+---------+-------------+------------------+-----------------+-------------+-----+------+------------+------------+--------------+--------------+------+\n",
            "|age|       workclass|fnlwgt|education|education_num|    marital_status|       occupation| relationship| race|   sex|capital_gain|capital_loss|hours_per_week|native_country|income|\n",
            "+---+----------------+------+---------+-------------+------------------+-----------------+-------------+-----+------+------------+------------+--------------+--------------+------+\n",
            "| 39|       State-gov| 77516|Bachelors|           13|     Never-married|     Adm-clerical|Not-in-family|White|  Male|        2174|           0|            40| United-States| <=50K|\n",
            "| 50|Self-emp-not-inc| 83311|Bachelors|           13|Married-civ-spouse|  Exec-managerial|      Husband|White|  Male|           0|           0|            13| United-States| <=50K|\n",
            "| 38|         Private|215646|  HS-grad|            9|          Divorced|Handlers-cleaners|Not-in-family|White|  Male|           0|           0|            40| United-States| <=50K|\n",
            "| 53|         Private|234721|     11th|            7|Married-civ-spouse|Handlers-cleaners|      Husband|Black|  Male|           0|           0|            40| United-States| <=50K|\n",
            "| 28|         Private|338409|Bachelors|           13|Married-civ-spouse|   Prof-specialty|         Wife|Black|Female|           0|           0|            40|          Cuba| <=50K|\n",
            "+---+----------------+------+---------+-------------+------------------+-----------------+-------------+-----+------+------------+------------+--------------+--------------+------+\n",
            "only showing top 5 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 3: Feature Engineering"
      ],
      "metadata": {
        "id": "Y09ZaSX15xGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "label_indexer = StringIndexer(inputCol=\"income\", outputCol=\"label\")\n",
        "\n",
        "categorical_cols = [\n",
        "    \"workclass\", \"education\", \"marital_status\",\n",
        "    \"occupation\", \"relationship\", \"race\",\n",
        "    \"sex\", \"native_country\"\n",
        "]\n",
        "\n",
        "indexers = [\n",
        "    StringIndexer(inputCol=col, outputCol=f\"{col}_idx\", handleInvalid=\"skip\")\n",
        "    for col in categorical_cols\n",
        "]\n",
        "\n",
        "encoders = [\n",
        "    OneHotEncoder(inputCol=f\"{col}_idx\", outputCol=f\"{col}_ohe\")\n",
        "    for col in categorical_cols\n",
        "]\n",
        "\n",
        "numeric_cols = [\n",
        "    \"age\", \"fnlwgt\", \"education_num\",\n",
        "    \"capital_gain\", \"capital_loss\", \"hours_per_week\"\n",
        "]\n",
        "\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=numeric_cols + [f\"{col}_ohe\" for col in categorical_cols],\n",
        "    outputCol=\"features\"\n",
        ")\n",
        "\n",
        "pipeline = Pipeline(\n",
        "    stages=indexers + encoders + [label_indexer, assembler]\n",
        ")\n",
        "\n",
        "fe_df = pipeline.fit(spark_df).transform(spark_df)\n",
        "final_df = fe_df.select(\"features\", \"label\")\n",
        "train_df, test_df = final_df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "display(final_df)\n",
        "final_df.show(5)\n",
        "print(\"Training rows:\", train_df.count())\n",
        "print(\"Testing rows:\", test_df.count())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "gpRMAFct3eYg",
        "outputId": "42c5d5d2-d79e-4690-de78-bd2386140a91"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[features: vector, label: double]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+\n",
            "|            features|label|\n",
            "+--------------------+-----+\n",
            "|(96,[0,1,2,3,5,9,...|  0.0|\n",
            "|(96,[0,1,2,5,7,14...|  0.0|\n",
            "|(96,[0,1,2,5,6,12...|  0.0|\n",
            "|(96,[0,1,2,5,6,17...|  0.0|\n",
            "|(96,[0,1,2,5,6,14...|  0.0|\n",
            "+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "Training rows: 24254\n",
            "Testing rows: 5908\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 4: Model Training"
      ],
      "metadata": {
        "id": "VsE1gU9B522E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression, GBTClassifier\n",
        "\n",
        "lr = LogisticRegression(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"label\",\n",
        "    maxIter=100\n",
        ")\n",
        "\n",
        "lr_model = lr.fit(train_df)\n",
        "\n",
        "print(\"Logistic Regression Model\")\n",
        "print(\"Iterations:\", lr_model.getMaxIter())\n",
        "print(\"Regularization:\", lr_model.getRegParam())\n",
        "print(\"Elastic Net:\", lr_model.getElasticNetParam())\n",
        "\n",
        "\n",
        "gbt = GBTClassifier(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"label\",\n",
        "    maxIter=20,\n",
        "    maxDepth=5\n",
        ")\n",
        "\n",
        "gbt_model = gbt.fit(train_df)\n",
        "\n",
        "print(\"\\nGradient Boosted Tree Model\")\n",
        "print(\"Iterations:\", gbt_model.getMaxIter())\n",
        "print(\"Tree Depth:\", gbt_model.getMaxDepth())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jd-eVxc73tP6",
        "outputId": "b9ce9446-03fc-4f09-aff8-78ef6e5038a7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Model\n",
            "Iterations: 100\n",
            "Regularization: 0.0\n",
            "Elastic Net: 0.0\n",
            "\n",
            "Gradient Boosted Tree Model\n",
            "Iterations: 20\n",
            "Tree Depth: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 5: Tuning and Evaluating"
      ],
      "metadata": {
        "id": "AhNgnt0N56H9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.classification import LogisticRegression, GBTClassifier\n",
        "\n",
        "evaluator = BinaryClassificationEvaluator(\n",
        "    labelCol=\"label\",\n",
        "    metricName=\"areaUnderROC\"\n",
        ")\n",
        "\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
        "\n",
        "lr_param_grid = (\n",
        "    ParamGridBuilder()\n",
        "    .addGrid(lr.regParam, [0.01, 0.1])\n",
        "    .addGrid(lr.elasticNetParam, [0.0, 0.5])\n",
        "    .build()\n",
        ")\n",
        "\n",
        "lr_cv = CrossValidator(\n",
        "    estimator=lr,\n",
        "    estimatorParamMaps=lr_param_grid,\n",
        "    evaluator=evaluator,\n",
        "    numFolds=5\n",
        ")\n",
        "\n",
        "lr_cv_model = lr_cv.fit(train_df)\n",
        "lr_best_auc = evaluator.evaluate(lr_cv_model.bestModel.transform(train_df))\n",
        "print(\"Best Logistic Regression AUC (Training):\", lr_best_auc)\n",
        "\n",
        "\n",
        "gbt = GBTClassifier(featuresCol=\"features\", labelCol=\"label\")\n",
        "\n",
        "gbt_param_grid = (\n",
        "    ParamGridBuilder()\n",
        "    .addGrid(gbt.maxDepth, [3, 5])\n",
        "    .addGrid(gbt.maxIter, [10, 20])\n",
        "    .build()\n",
        ")\n",
        "\n",
        "gbt_cv = CrossValidator(\n",
        "    estimator=gbt,\n",
        "    estimatorParamMaps=gbt_param_grid,\n",
        "    evaluator=evaluator,\n",
        "    numFolds=5\n",
        ")\n",
        "\n",
        "gbt_cv_model = gbt_cv.fit(train_df)\n",
        "gbt_best_auc = evaluator.evaluate(gbt_cv_model.bestModel.transform(train_df))\n",
        "print(\"Best GBT AUC (Training):\", gbt_best_auc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxkvG_kn4oEp",
        "outputId": "61310782-eb9c-48ef-8373-05d77dcb71c5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Logistic Regression AUC (Training): 0.9040112001567027\n",
            "Best GBT AUC (Training): 0.9180995307160787\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 6: Prediction and Final Evaluation"
      ],
      "metadata": {
        "id": "zhvXGQSK5-7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "evaluator = BinaryClassificationEvaluator(\n",
        "    labelCol=\"label\",\n",
        "    metricName=\"areaUnderROC\"\n",
        ")\n",
        "\n",
        "lr_test_predictions = lr_cv_model.bestModel.transform(test_df)\n",
        "lr_test_auc = evaluator.evaluate(lr_test_predictions)\n",
        "print(\"Logistic Regression Test AUC:\", lr_test_auc)\n",
        "\n",
        "gbt_test_predictions = gbt_cv_model.bestModel.transform(test_df)\n",
        "gbt_test_auc = evaluator.evaluate(gbt_test_predictions)\n",
        "print(\"Gradient Boosted Tree Test AUC:\", gbt_test_auc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-RERUns4zMt",
        "outputId": "f9378ab6-6a25-4bd2-baf7-8a5dd90997fb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Test AUC: 0.9001087913915462\n",
            "Gradient Boosted Tree Test AUC: 0.9050750837686011\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VKWvxF_A5EJp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}